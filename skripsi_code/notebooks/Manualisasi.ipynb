{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.346083Z",
     "start_time": "2024-11-25T05:50:25.799953Z"
    }
   },
   "source": [
    "# Example input matrix X (normalized values from the table)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "X = np.array([\n",
    "    [-0.577, -0.864, -0.629, -0.719, 0.134, 0.818],\n",
    "    [1.732, -0.890, -0.551, 1.715, -0.230, -0.719],\n",
    "    [-0.577, 1.555, 1.731, -0.350, -1.352, -1.234],\n",
    "    [-0.577, 0.199, -0.551, -0.645, 1.448, 1.135]\n",
    "])\n",
    "\n",
    "# Example weight matrix W (given in the table)\n",
    "W = np.array([\n",
    "    [0.12, -0.45, 0.88, -0.22, 0.78, 0.15],\n",
    "    [-0.61, 0.29, -0.78, 0.04, 0.83, -0.16],\n",
    "    [0.33, -0.72, 0.90, 0.18, 0.47, -0.11],\n",
    "    [0.62, -0.36, -0.57, 0.89, -0.09, 0.72],\n",
    "    [0.50, -0.14, -0.21, 0.68, 0.44, -0.77],\n",
    "    [-0.85, 0.35, -0.19, 0.49, -0.63, 0.11]\n",
    "]).T  # Transposed to match input dimensions\n",
    "\n",
    "# Example bias vector b (given in the table)\n",
    "b = np.array([0.15, -0.24, 0.31, -0.72, 0.49, -0.63])\n",
    "\n",
    "# Perform the linear transformation: Y = X @ W + b\n",
    "Y = X @ W + b\n",
    "\n",
    "Y\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30144,  0.30361,  0.01915, -0.47118, -0.60527, -0.66919],\n",
       "       [-0.39109, -1.1321 ,  1.30615,  2.01768,  3.21494, -1.40285],\n",
       "       [-0.25837, -1.72598, -0.00481, -3.70251, -0.26241,  0.62033],\n",
       "       [ 0.94792,  1.5939 , -0.07998, -0.72248, -0.38608, -1.06865]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.408377Z",
     "start_time": "2024-11-25T05:50:26.396383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Small constant to avoid division by zero\n",
    "delta = 1e-5\n",
    "\n",
    "# Example feature matrix (assume Y is already provided)\n",
    "df = pd.DataFrame(Y)\n",
    "\n",
    "# Initialize dictionaries for normalized data and scaled/shifted data\n",
    "normalized_data = {}\n",
    "scaled_shifted_data = {}\n",
    "\n",
    "# Define gamma (scale) and beta (shift) for each feature\n",
    "# These can be initialized randomly or with specific values\n",
    "gamma = {feature: 1.0 for feature in df.columns}  # Example: All scale factors are 1.0\n",
    "beta = {feature: 0.0 for feature in df.columns}   # Example: All shift values are 0.0\n",
    "\n",
    "for feature in df.columns:\n",
    "    H = df[feature].values\n",
    "    mu = np.mean(H)\n",
    "    sigma = np.sqrt(delta + np.mean((H - mu) ** 2))\n",
    "    normalized_data[feature] = (H - mu) / sigma\n",
    "\n",
    "    # Apply scaling and shifting\n",
    "    scaled_shifted_data[feature] = gamma[feature] * normalized_data[feature] + beta[feature]\n",
    "    print(f\"{feature}: {mu=}, {sigma=}, gamma={gamma[feature]}, beta={beta[feature]}\")\n",
    "\n",
    "# Create DataFrames for normalized and scaled/shifted values\n",
    "normalized_df = pd.DataFrame(normalized_data)\n",
    "scaled_shifted_df = pd.DataFrame(scaled_shifted_data)\n",
    "\n",
    "# Display the scaled and shifted results to the user\n",
    "print(scaled_shifted_df)\n"
   ],
   "id": "28dec40edd7cee03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: mu=np.float64(0.14997499999999997), sigma=np.float64(0.5289600274359113), gamma=1.0, beta=0.0\n",
      "1: mu=np.float64(-0.2401425000000001), sigma=np.float64(1.2906122511501081), gamma=1.0, beta=0.0\n",
      "2: mu=np.float64(0.31012750000000006), sigma=np.float64(0.5762243956296453), gamma=1.0, beta=0.0\n",
      "3: mu=np.float64(-0.7196224999999999), sigma=np.float64(2.028066484072638), gamma=1.0, beta=0.0\n",
      "4: mu=np.float64(0.49029500000000004), sigma=np.float64(1.5778617654043716), gamma=1.0, beta=0.0\n",
      "5: mu=np.float64(-0.6300899999999999), sigma=np.float64(0.7672372315001403), gamma=1.0, beta=0.0\n",
      "          0         1         2         3         4         5\n",
      "0  0.286345  0.421314 -0.504973  0.122502 -0.694335 -0.050962\n",
      "1 -1.022884 -0.691112  1.728532  1.349710  1.726796 -1.007198\n",
      "2 -0.771977 -1.151266 -0.546554 -1.470804 -0.477041  1.629770\n",
      "3  1.508517  1.421064 -0.677006 -0.001409 -0.555419 -0.571609\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.671860488Z",
     "start_time": "2024-11-23T14:24:03.352764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Small constant to avoid division by zero\n",
    "delta = 1e-5\n",
    "\n",
    "# Example feature matrix (assume Y is already provided)\n",
    "df = pd.DataFrame(Y)\n",
    "\n",
    "# Initialize dictionaries for normalized data and scaled/shifted data\n",
    "normalized_data = {}\n",
    "scaled_shifted_data = {}\n",
    "\n",
    "# Define unique gamma (scale) and beta (shift) for each feature\n",
    "# These can be initialized randomly or with specific values\n",
    "gamma = {feature: np.random.uniform(0.5, 1.5) for feature in df.columns}  # Random scale factors\n",
    "beta = {feature: np.random.uniform(-1.0, 1.0) for feature in df.columns}  # Random shift values\n",
    "\n",
    "for feature in df.columns:\n",
    "    H = df[feature].values\n",
    "    mu = np.mean(H)\n",
    "    sigma = np.sqrt(delta + np.mean((H - mu) ** 2))\n",
    "    normalized_data[feature] = (H - mu) / sigma\n",
    "\n",
    "    # Apply unique scaling and shifting\n",
    "    scaled_shifted_data[feature] = gamma[feature] * normalized_data[feature] + beta[feature]\n",
    "    print(f\"{feature}: {mu=}, {sigma=}, gamma={gamma[feature]}, beta={beta[feature]}\")\n",
    "\n",
    "# Create DataFrames for normalized and scaled/shifted values\n",
    "normalized_df = pd.DataFrame(normalized_data)\n",
    "scaled_shifted_df = pd.DataFrame(scaled_shifted_data)\n",
    "\n",
    "# Display the scaled and shifted results to the user\n",
    "print(scaled_shifted_df)\n"
   ],
   "id": "11cc119fbdfe5428",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: mu=np.float64(0.14997499999999997), sigma=np.float64(0.5289600274359113), gamma=0.9523207609381628, beta=0.6740618945497125\n",
      "1: mu=np.float64(-0.2401425000000001), sigma=np.float64(1.2906122511501081), gamma=0.9238090512247757, beta=0.4798346794985424\n",
      "2: mu=np.float64(0.31012750000000006), sigma=np.float64(0.5762243956296453), gamma=1.4488933260475712, beta=-0.18634117178885923\n",
      "3: mu=np.float64(-0.7196224999999999), sigma=np.float64(2.028066484072638), gamma=0.9999455218891956, beta=-0.6507961444911954\n",
      "4: mu=np.float64(0.49029500000000004), sigma=np.float64(1.5778617654043716), gamma=0.9218509014156092, beta=-0.48669323687063604\n",
      "5: mu=np.float64(-0.6300899999999999), sigma=np.float64(0.7672372315001403), gamma=1.2317505341593744, beta=0.9226083758243546\n",
      "          0         1         2         3         4         5\n",
      "0  0.946754  0.869048 -0.917993 -0.528301 -1.126767  0.859836\n",
      "1 -0.300052 -0.158621  2.318118  0.698841  1.105155 -0.318009\n",
      "2 -0.061108 -0.583715 -0.978239 -2.121520 -0.926454  2.930078\n",
      "3  2.110654  1.792626 -1.167251 -0.652205 -0.998707  0.218528\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.697084927Z",
     "start_time": "2024-11-23T14:23:57.399082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming scaled_shifted_df is the DataFrame containing the scaled and shifted data\n",
    "# Define the ELU activation function with alpha parameter\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "# Apply ELU activation to each column of the DataFrame\n",
    "alpha = 1.0  # You can change this based on your requirements\n",
    "elu_df = scaled_shifted_df.map(lambda x: elu(x, alpha))\n",
    "\n",
    "# Display the ELU-applied DataFrame\n",
    "print(elu_df)\n"
   ],
   "id": "136c0132f5875984",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5\n",
      "0  0.384782  1.312476 -0.512450 -0.279354  0.583666  0.814915\n",
      "1 -0.669198 -0.182337  2.622936  0.494959  1.801032  0.285173\n",
      "2 -0.559783 -0.562850 -0.541854 -0.752306  0.692924  1.746018\n",
      "3  1.776653  2.672930 -0.623080 -0.336789  0.653514  0.526483\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:45.385576Z",
     "start_time": "2024-11-25T05:50:45.362450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_dropout(dataframe, dropout_rate=0.5):\n",
    "    # Generate a random mask with the same shape as the DataFrame\n",
    "    mask = np.random.rand(*dataframe.shape) > dropout_rate\n",
    "    # Apply the mask and scale the remaining values\n",
    "    print(dataframe * mask)\n",
    "    return dataframe * mask / (1 - dropout_rate)\n",
    "\n",
    "# Step 2: Apply dropout\n",
    "dropout_rate = 0.25  # Adjust this as needed\n",
    "dropout_df = apply_dropout(elu_df, dropout_rate)\n",
    "\n",
    "# Display the resulting DataFrame after ELU and dropout\n",
    "print(dropout_df)"
   ],
   "id": "bf151f7dd75ba2c0",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'elu_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Step 2: Apply dropout\u001B[39;00m\n\u001B[1;32m      9\u001B[0m dropout_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.25\u001B[39m  \u001B[38;5;66;03m# Adjust this as needed\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m dropout_df \u001B[38;5;241m=\u001B[39m apply_dropout(\u001B[43melu_df\u001B[49m, dropout_rate)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Display the resulting DataFrame after ELU and dropout\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(dropout_df)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'elu_df' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.699174663Z",
     "start_time": "2024-11-24T06:58:50.924912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Data Setup\n",
    "# True labels (one-hot encoded)\n",
    "y_true = np.array([\n",
    "    [0, 1, 0],  # True label for x1\n",
    "    [1, 0, 0],  # True label for x2\n",
    "    [0, 0, 1],  # True label for x3\n",
    "    [0, 0, 1],  # True label for x4\n",
    "    [0, 1, 0]   # True label for x5\n",
    "])\n",
    "\n",
    "# Predicted logits (before softmax)\n",
    "logits = np.array([\n",
    "    [1.23, 2.11, 0.87],  # Predictions for x1\n",
    "    [0.91, 1.34, 0.76],  # Predictions for x2\n",
    "    [1.45, 0.88, 2.31],  # Predictions for x3\n",
    "    [1.07, 0.94, 1.56],  # Predictions for x4\n",
    "    [0.78, 1.65, 0.94]   # Predictions for x5\n",
    "])\n",
    "\n",
    "# Step 2: Apply Softmax to Convert Logits to Probabilities\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits)  # Subtract max for numerical stability\n",
    "    # print(exp_logits)\n",
    "    su = np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    # print(su)\n",
    "    probabilities = exp_logits / su\n",
    "    return probabilities\n",
    "\n",
    "# Compute softmax probabilities\n",
    "probabilities = softmax(logits)\n",
    "# print(probabilities)\n",
    "\n",
    "# Step 3: Compute Cross-Entropy Loss\n",
    "def cross_entropy_loss(y_true, probabilities):\n",
    "    # Select the probabilities corresponding to the true labels\n",
    "    true_probs = np.sum(y_true * probabilities, axis=1)\n",
    "    # print(true_probs)\n",
    "    # Compute the negative log of these probabilities\n",
    "    log_loss = -np.log(true_probs)\n",
    "    # Return the mean loss across all examples\n",
    "    print(log_loss)\n",
    "    return np.mean(log_loss)\n",
    "\n",
    "# Calculate the cross-entropy loss\n",
    "loss = cross_entropy_loss(y_true, probabilities)\n",
    "\n",
    "# Print the results\n",
    "# print(\"Softmax Probabilities:\")\n",
    "# print(probabilities)\n",
    "print(\"\\nCross-Entropy Loss:\")\n",
    "print(loss)\n"
   ],
   "id": "a0b061ca629296b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5330765  1.22317687 0.50830505 0.76573331 0.6474151 ]\n",
      "\n",
      "Cross-Entropy Loss:\n",
      "0.7355413679599727\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.727452614Z",
     "start_time": "2024-11-24T07:14:25.695454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Softmax function\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "# Entropy loss function\n",
    "def entropy_loss(probabilities, lambda_weight=0.1):\n",
    "    # Compute entropy: -sum(p * log(p))\n",
    "    entropy = -np.sum(probabilities * np.log(probabilities), axis=1)\n",
    "    # print(np.log(probabilities))\n",
    "    print(-np.sum(probabilities * np.log(probabilities), axis=1))\n",
    "    # Avoid log(0) with 1e-12\n",
    "    print(np.mean(entropy))\n",
    "    return lambda_weight * np.mean(entropy)\n",
    "\n",
    "# Compute probabilities (softmax)\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "# Compute entropy loss\n",
    "lambda_weight = 0.1\n",
    "loss = entropy_loss(probabilities, lambda_weight)\n",
    "\n",
    "print(\"Probabilities:\", probabilities)\n",
    "print(\"Entropy Loss:\", loss)\n"
   ],
   "id": "d291fc48f2f7031f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95782674 1.06663775 0.933053   1.06040507 1.02088774]\n",
      "1.007762058965028\n",
      "Probabilities: [[0.24339333 0.58679691 0.16980976]\n",
      " [0.29429375 0.45240528 0.25330098]\n",
      " [0.25453802 0.14394773 0.60151425]\n",
      " [0.28486688 0.2501403  0.46499282]\n",
      " [0.21927797 0.52339696 0.25732508]]\n",
      "Entropy Loss: 0.10077620589650281\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.728286145Z",
     "start_time": "2024-11-24T08:42:30.483734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Creating the numpy array based on the table provided\n",
    "data = np.array([\n",
    "    [0.384, 1.312, -0.718, -0.327, 0.583, 0.814],  # x1\n",
    "    [-1.106, -0.201, 2.622, 0.494, 1.801, 0.285],  # x2\n",
    "    [-0.820, -0.827, -0.780, -1.395, 0.692, 1.746],  # x3\n",
    "    [1.776, 2.672, -0.975, -0.410, 0.653, 0.526],  # x4\n",
    "    [0.632, 1.242, -0.394, 0.434, 0.465, 0.696],  # x5\n",
    "    [-0.316, 0.182, 2.387, 0.765, 1.569, 0.052],  # x6\n",
    "    [-0.699, -1.783, -1.642, -1.676, 0.185, 1.903],  # x7\n",
    "    [1.321, 1.965, -0.242, -0.522, 0.686, -0.186],  # x8\n",
    "])\n",
    "\n",
    "k = 3\n",
    "\n",
    "# Step 1: Initialize centroids randomly (from data points)\n",
    "np.random.seed(1213)  # For reproducibility\n",
    "initial_indices = np.random.choice(data.shape[0], k, replace=False)\n",
    "centroids = data[initial_indices]\n",
    "# print(centroids)\n",
    "\n",
    "# Function to compute Euclidean distance\n",
    "def compute_distances(data, centroids):\n",
    "    distances = np.zeros((data.shape[0], k))\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        distances[:, i] = np.linalg.norm(data - centroid, axis=1)\n",
    "    return distances\n",
    "\n",
    "# Iterative K-means clustering\n",
    "max_iterations = 2\n",
    "for iteration in range(max_iterations):\n",
    "    # Step 2: Compute distances and assign clusters\n",
    "    distances = compute_distances(data, centroids)\n",
    "    # print(distances.T)\n",
    "\n",
    "    cluster_assignments = np.argmin(distances, axis=1)\n",
    "    # print(cluster_assignments)\n",
    "\n",
    "    # Step 3: Update centroids\n",
    "    new_centroids = np.array([data[cluster_assignments == i].mean(axis=0) for i in range(k)])\n",
    "    # print(new_centroids)\n",
    "\n",
    "    # Check for convergence\n",
    "    if np.allclose(centroids, new_centroids):\n",
    "        break\n",
    "    centroids = new_centroids\n",
    "\n",
    "# Prepare final results for display\n",
    "cluster_table = pd.DataFrame(data, columns=[f\"z_{i+1}\" for i in range(data.shape[1])])\n",
    "cluster_table[\"Cluster\"] = cluster_assignments + 1  # Clusters 1-indexed\n",
    "\n",
    "centroid_table = pd.DataFrame(centroids, columns=[f\"z_{i+1}\" for i in range(data.shape[1])])\n",
    "centroid_table[\"Cluster\"] = range(1, k + 1)  # Clusters 1-indexed\n",
    "\n",
    "# Display the results\n",
    "print(centroid_table)\n",
    "print(cluster_table)"
   ],
   "id": "233695c631f32ec7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       z_1      z_2      z_3      z_4      z_5     z_6  Cluster\n",
      "0 -0.71100 -0.00950  2.50450  0.62950  1.68500  0.1685        1\n",
      "1  1.02825  1.79775 -0.58225 -0.20625  0.59675  0.4625        2\n",
      "2 -0.75950 -1.30500 -1.21100 -1.53550  0.43850  1.8245        3\n",
      "     z_1    z_2    z_3    z_4    z_5    z_6  Cluster\n",
      "0  0.384  1.312 -0.718 -0.327  0.583  0.814        2\n",
      "1 -1.106 -0.201  2.622  0.494  1.801  0.285        1\n",
      "2 -0.820 -0.827 -0.780 -1.395  0.692  1.746        3\n",
      "3  1.776  2.672 -0.975 -0.410  0.653  0.526        2\n",
      "4  0.632  1.242 -0.394  0.434  0.465  0.696        2\n",
      "5 -0.316  0.182  2.387  0.765  1.569  0.052        1\n",
      "6 -0.699 -1.783 -1.642 -1.676  0.185  1.903        3\n",
      "7  1.321  1.965 -0.242 -0.522  0.686 -0.186        2\n"
     ]
    }
   ],
   "execution_count": 190
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.743219967Z",
     "start_time": "2024-11-24T08:56:37.592496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Updated Frequency Matrix based on the provided image\n",
    "frequency_matrix = np.array([\n",
    "    [2, 0, 0],  # C1 overlaps with Ĉ1, Ĉ2, Ĉ3\n",
    "    [1, 1, 2],  # C2 overlaps with Ĉ1, Ĉ2, Ĉ3\n",
    "    [0, 0, 3],  # C3 overlaps with Ĉ1, Ĉ2, Ĉ3\n",
    "])\n",
    "\n",
    "# Step 1: Convert to cost matrix (maximize overlap by minimizing cost)\n",
    "max_value = np.max(frequency_matrix)\n",
    "cost_matrix = max_value - frequency_matrix\n",
    "print(cost_matrix)\n",
    "\n",
    "# Step 2: Apply Kuhn-Munkres (Hungarian algorithm) using linear_sum_assignment\n",
    "row_indices, col_indices = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "# Step 3: Prepare the results\n",
    "assignments = list(zip(row_indices + 1, col_indices + 1))  # Clusters 1-indexed\n",
    "\n",
    "# Output the updated frequency matrix, cost matrix, and assignments\n",
    "frequency_df = pd.DataFrame(\n",
    "    frequency_matrix,\n",
    "    index=[\"C1\", \"C2\", \"C3\"],\n",
    "    columns=[\"Ĉ1\", \"Ĉ2\", \"Ĉ3\"]\n",
    ")\n",
    "cost_df = pd.DataFrame(\n",
    "    cost_matrix,\n",
    "    index=[\"C1\", \"C2\", \"C3\"],\n",
    "    columns=[\"Ĉ1\", \"Ĉ2\", \"Ĉ3\"]\n",
    ")\n",
    "\n"
   ],
   "id": "337794ac4e1d44dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 3]\n",
      " [2 2 1]\n",
      " [3 3 0]]\n"
     ]
    }
   ],
   "execution_count": 192
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.744945718Z",
     "start_time": "2024-11-24T08:58:49.048516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step-by-step demonstration of Kuhn-Munkres algorithm (Hungarian Algorithm)\n",
    "\n",
    "steps = []  # To store each step for visualization\n",
    "\n",
    "# Step 1: Subtract the row minimum\n",
    "row_min = cost_matrix.min(axis=1)\n",
    "step1_matrix = cost_matrix - row_min[:, np.newaxis]\n",
    "steps.append((\"Step 1: Subtract Row Minimum\", step1_matrix))\n",
    "\n",
    "# Step 2: Subtract the column minimum\n",
    "col_min = step1_matrix.min(axis=0)\n",
    "step2_matrix = step1_matrix - col_min\n",
    "steps.append((\"Step 2: Subtract Column Minimum\", step2_matrix))\n",
    "\n",
    "# Step 3: Cover all zeros with minimum number of lines\n",
    "# (This step will only be conceptually described since visualization is complex)\n",
    "\n",
    "# Display each step as a DataFrame\n",
    "for step_name, matrix in steps:\n",
    "    step_df = pd.DataFrame(matrix, index=[\"C1\", \"C2\", \"C3\"], columns=[\"Ĉ1\", \"Ĉ2\", \"Ĉ3\"])\n",
    "\n",
    "# Final Step: Perform assignment using linear_sum_assignment\n",
    "assignments\n"
   ],
   "id": "22d7250e2351a062",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.int64(1), np.int64(1)),\n",
       " (np.int64(2), np.int64(2)),\n",
       " (np.int64(3), np.int64(3))]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.750559945Z",
     "start_time": "2024-11-24T09:05:30.161616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Updated cost matrix based on the uploaded image\n",
    "cost_matrix = np.array([\n",
    "    [0, 1, 2],  # C1\n",
    "    [1, 0, 0],  # C2\n",
    "    [3, 2, 0],  # C3\n",
    "])\n",
    "\n",
    "# Visualizing the matrix with \"crossed out zeros\"\n",
    "def visualize_matrix_with_zeros(matrix, covered_rows, covered_cols):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.matshow(matrix, cmap=\"coolwarm\", alpha=0.2)\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            c = matrix[i, j]\n",
    "            ax.text(x=j, y=i, s=str(c), va='center', ha='center')\n",
    "\n",
    "    # Add lines to \"cross out zeros\"\n",
    "    for row in covered_rows:\n",
    "        ax.plot([-0.5, matrix.shape[1] - 0.5], [row, row], color=\"black\", linewidth=2)\n",
    "    for col in covered_cols:\n",
    "        ax.plot([col, col], [-0.5, matrix.shape[0] - 0.5], color=\"black\", linewidth=2)\n",
    "\n",
    "    plt.xticks(range(matrix.shape[1]), [f\"Ĉ{j+1}\" for j in range(matrix.shape[1])])\n",
    "    plt.yticks(range(matrix.shape[0]), [f\"C{i+1}\" for i in range(matrix.shape[0])])\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "# Simulating minimum cover lines (this can be done programmatically)\n",
    "covered_rows = [0, 1]  # Example: Covering row 1\n",
    "covered_cols = [2]  # Example: Covering column 3\n",
    "\n",
    "# Visualize the matrix with crossed out zeros\n",
    "visualize_matrix_with_zeros(cost_matrix, covered_rows, covered_cols)\n"
   ],
   "id": "f92e1d3ac09e6e48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.752140575Z",
     "start_time": "2024-11-24T12:39:34.583472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Provided gradients for Cross-Entropy (CE) and Entropy (E) Loss\n",
    "cross_entropy_gradients_table = np.array([\n",
    "    [0.243, -0.414, 0.169],  # x1\n",
    "    [-0.706, 0.452, 0.253],  # x2\n",
    "    [0.254, 0.143, -0.399],  # x3\n",
    "    [0.284, 0.250, -0.536],  # x4\n",
    "    [0.219, -0.477, 0.257],  # x5\n",
    "])\n",
    "\n",
    "entropy_gradients_table = np.array([\n",
    "    [0.082, -0.093, 0.155],  # x1\n",
    "    [0.044, -0.041, 0.074],  # x2\n",
    "    [0.074, 0.188, -0.098],  # x3\n",
    "    [0.051, 0.077, -0.046],  # x4\n",
    "    [0.103, -0.070, 0.717],  # x5\n",
    "])\n",
    "\n",
    "# Summing the gradients element-wise\n",
    "summed_gradients_table = cross_entropy_gradients_table + entropy_gradients_table\n",
    "\n",
    "# Create a DataFrame for the summed gradients\n",
    "summed_gradients_df = pd.DataFrame(\n",
    "    summed_gradients_table,\n",
    "    columns=[\"dL/dy1 (Summed)\", \"dL/dy2 (Summed)\", \"dL/dy3 (Summed)\"],\n",
    "    index=[f\"x{i+1}\" for i in range(len(summed_gradients_table))]\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "summed_gradients_df"
   ],
   "id": "83dcf899cb2db94",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    dL/dy1 (Summed)  dL/dy2 (Summed)  dL/dy3 (Summed)\n",
       "x1            0.325           -0.507            0.324\n",
       "x2           -0.662            0.411            0.327\n",
       "x3            0.328            0.331           -0.497\n",
       "x4            0.335            0.327           -0.582\n",
       "x5            0.322           -0.547            0.974"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dL/dy1 (Summed)</th>\n",
       "      <th>dL/dy2 (Summed)</th>\n",
       "      <th>dL/dy3 (Summed)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>0.324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>-0.662</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3</th>\n",
       "      <td>0.328</td>\n",
       "      <td>0.331</td>\n",
       "      <td>-0.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x4</th>\n",
       "      <td>0.335</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x5</th>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>0.974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 221
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.754239789Z",
     "start_time": "2024-11-24T12:39:58.479970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "averaged_gradients = np.mean(summed_gradients_table, axis=0)\n",
    "\n",
    "# Create a DataFrame for the averaged gradients\n",
    "averaged_gradients_df = pd.DataFrame(\n",
    "    [averaged_gradients],\n",
    "    columns=[\"Average(dL/dy1)\", \"Average(dL/dy2)\", \"Average(dL/dy3)\"],\n",
    "    index=[\"Batch Average\"]\n",
    ")\n",
    "\n",
    "averaged_gradients_df"
   ],
   "id": "1de0818c9945b9f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               Average(dL/dy1)  Average(dL/dy2)  Average(dL/dy3)\n",
       "Batch Average           0.1296            0.003           0.1092"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average(dL/dy1)</th>\n",
       "      <th>Average(dL/dy2)</th>\n",
       "      <th>Average(dL/dy3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Batch Average</th>\n",
       "      <td>0.1296</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.1092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.755844335Z",
     "start_time": "2024-11-24T13:54:14.521264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the given tables (partial derivatives)\n",
    "dL_dy_hat = np.array([\n",
    "    [0.325, -0.507, 0.324],  # x1\n",
    "    [-0.661, 0.411, 0.327],  # x2\n",
    "    [0.328, 0.330, -0.497],  # x3\n",
    "    [0.334, 0.327, -0.582],  # x4\n",
    "    [0.322, -0.546, 0.974],  # x5\n",
    "])\n",
    "\n",
    "dy_hat_dz = np.array([\n",
    "    [1.0, 1.0, 0.487],  # x1\n",
    "    [0.330, 0.817, 1.0],  # x2\n",
    "    [0.440, 0.437, 0.458],  # x3\n",
    "    [1.0, 1.0, 0.377],  # x4\n",
    "    [1.0, 1.0, 0.785],  # x5\n",
    "])\n",
    "\n",
    "# Multiply the derivatives to get ∂L_total/∂z\n",
    "dL_dz = dL_dy_hat * dy_hat_dz  # Element-wise multiplication\n",
    "\n",
    "# Convert to a DataFrame for better readability\n",
    "dL_dz_df = pd.DataFrame(\n",
    "    dL_dz,\n",
    "    columns=[f\"∂L/∂z_{i+1}\" for i in range(dL_dz.shape[1])],\n",
    "    index=[f\"x_{i+1}\" for i in range(dL_dz.shape[0])]\n",
    ")\n",
    "\n",
    "print(dL_dz_df.mean())\n",
    "dL_dz_df\n"
   ],
   "id": "9367136902c3b112",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∂L/∂z_1    0.181438\n",
      "∂L/∂z_2   -0.049201\n",
      "∂L/∂z_3    0.160468\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     ∂L/∂z_1   ∂L/∂z_2   ∂L/∂z_3\n",
       "x_1  0.32500 -0.507000  0.157788\n",
       "x_2 -0.21813  0.335787  0.327000\n",
       "x_3  0.14432  0.144210 -0.227626\n",
       "x_4  0.33400  0.327000 -0.219414\n",
       "x_5  0.32200 -0.546000  0.764590"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>∂L/∂z_1</th>\n",
       "      <th>∂L/∂z_2</th>\n",
       "      <th>∂L/∂z_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x_1</th>\n",
       "      <td>0.32500</td>\n",
       "      <td>-0.507000</td>\n",
       "      <td>0.157788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_2</th>\n",
       "      <td>-0.21813</td>\n",
       "      <td>0.335787</td>\n",
       "      <td>0.327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_3</th>\n",
       "      <td>0.14432</td>\n",
       "      <td>0.144210</td>\n",
       "      <td>-0.227626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_4</th>\n",
       "      <td>0.33400</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>-0.219414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_5</th>\n",
       "      <td>0.32200</td>\n",
       "      <td>-0.546000</td>\n",
       "      <td>0.764590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 225
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.757444265Z",
     "start_time": "2024-11-24T15:30:10.431127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Re-import necessary libraries since the environment reset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the gradient matrices from the images\n",
    "dL_dz = np.array([\n",
    "    [0.325, -0.507, 0.157],  # x1\n",
    "    [-0.218, 0.335, 0.327],  # x2\n",
    "    [0.144, 0.144, -0.227],  # x3\n",
    "    [0.334, 0.327, -0.219],  # x4\n",
    "    [0.322, -0.546, 0.764],  # x5\n",
    "])\n",
    "\n",
    "dz_dw = np.array([\n",
    "    [-0.577, -0.864, -0.629],  # x1\n",
    "    [1.732, -0.890, -0.551],  # x2\n",
    "    [-0.577, 1.555, 1.731],   # x3\n",
    "    [-0.577, 0.199, -0.551],  # x4\n",
    "    [1.715, -0.230, -0.719],  # x5\n",
    "])\n",
    "\n",
    "dz_db = np.ones((5, 3))  # Bias gradient is a constant 1 for each element (as shown in the δw row).\n",
    "\n",
    "# Calculate ∂L/∂W (weights)\n",
    "dL_dW = dL_dz * dz_dw  # Element-wise multiplication\n",
    "\n",
    "# Calculate ∂L/∂b (biases)\n",
    "dL_db = dL_dz * dz_db  # Element-wise multiplication\n",
    "\n",
    "# Average gradients for weights and biases across all samples\n",
    "dL_dW_avg = np.mean(dL_dW, axis=0)\n",
    "dL_db_avg = np.mean(dL_db, axis=0)\n",
    "\n",
    "# Convert to DataFrames for better readability\n",
    "dL_dW_df = pd.DataFrame(\n",
    "    dL_dW, columns=[f\"∂L/∂w_{i+1}\" for i in range(dL_dW.shape[1])], index=[f\"x_{i+1}\" for i in range(dL_dW.shape[0])]\n",
    ")\n",
    "dL_db_df = pd.DataFrame(\n",
    "    dL_db, columns=[f\"∂L/∂b_{i+1}\" for i in range(dL_db.shape[1])], index=[f\"x_{i+1}\" for i in range(dL_db.shape[0])]\n",
    ")\n",
    "\n",
    "\n",
    "dL_dW_avg, dL_db_avg\n"
   ],
   "id": "8aabd04050134195",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.0577354,  0.1108942, -0.2201028]),\n",
       " array([ 0.1814, -0.0494,  0.1604]))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.758700054Z",
     "start_time": "2024-11-24T16:06:44.471052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute partial derivatives with respect to f (dL/df)\n",
    "# Formula: dL/df = W^T * dL/dz\n",
    "w_matrix = np.array([\n",
    "    [0.12, -0.45, 0.88],\n",
    "    [-0.61, 0.29, -0.78],\n",
    "    [0.33, -0.72, 0.90]\n",
    "])\n",
    "\n",
    "# Compute dL/df using matrix multiplication\n",
    "dL_df = np.dot(dL_dz, w_matrix.T)\n",
    "\n",
    "# Prepare dataframe for display\n",
    "dL_df_df = pd.DataFrame(dL_df, columns=[\"f1\", \"f2\", \"f3\"], index=[f\"x{i+1}\" for i in range(dL_dz.shape[0])])\n",
    "\n",
    "# Display the results for the user\n",
    "dL_df_df"
   ],
   "id": "79a5e23f57474ead",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         f1       f2       f3\n",
       "x1  0.40531 -0.46774  0.61359\n",
       "x2  0.11085 -0.02493 -0.01884\n",
       "x3 -0.24728  0.13098 -0.26046\n",
       "x4 -0.29979  0.06191 -0.32232\n",
       "x5  0.95666 -0.95068  1.18698"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>0.40531</td>\n",
       "      <td>-0.46774</td>\n",
       "      <td>0.61359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x2</th>\n",
       "      <td>0.11085</td>\n",
       "      <td>-0.02493</td>\n",
       "      <td>-0.01884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3</th>\n",
       "      <td>-0.24728</td>\n",
       "      <td>0.13098</td>\n",
       "      <td>-0.26046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x4</th>\n",
       "      <td>-0.29979</td>\n",
       "      <td>0.06191</td>\n",
       "      <td>-0.32232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x5</th>\n",
       "      <td>0.95666</td>\n",
       "      <td>-0.95068</td>\n",
       "      <td>1.18698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 230
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.760336791Z",
     "start_time": "2024-11-24T16:23:07.492123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Original weights\n",
    "weights = np.array([\n",
    "    [0.12, -0.45, 0.88],\n",
    "    [-0.61, 0.29, -0.78],\n",
    "    [0.33, -0.72, 0.90]\n",
    "])\n",
    "\n",
    "# Gradients\n",
    "gradients = np.array([-0.057, 0.110, -0.220])\n",
    "\n",
    "# Learning rate (alpha)\n",
    "alpha = 1\n",
    "\n",
    "# Update rule: w_new = w_old - alpha * gradient\n",
    "updated_weights = weights - alpha * gradients\n",
    "\n",
    "# Creating a DataFrame for better visualization\n",
    "weights_df = pd.DataFrame(updated_weights, columns=[\"w1\", \"w2\", \"w3\"], index=[\"f1\", \"f2\", \"f3\"])\n",
    "\n",
    "weights_df"
   ],
   "id": "280268607be4e8b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       w1    w2    w3\n",
       "f1  0.177 -0.56  1.10\n",
       "f2 -0.553  0.18 -0.56\n",
       "f3  0.387 -0.83  1.12"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2</th>\n",
       "      <td>-0.553</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f3</th>\n",
       "      <td>0.387</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 239
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:26.761950223Z",
     "start_time": "2024-11-24T16:29:48.217089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initial feature values\n",
    "f = np.array([\n",
    "    [-0.577, -0.864, -0.629],\n",
    "    [1.732, -0.890, -0.551],\n",
    "    [-0.577, 1.555, 1.731],\n",
    "    [-0.577, 0.199, -0.551],\n",
    "    [1.715, -0.230, -0.719]\n",
    "])\n",
    "\n",
    "# Gradient average for features\n",
    "grad_f = np.array([0.185, -0.250, 0.239])\n",
    "\n",
    "# Update rule for features\n",
    "learning_rate = 1\n",
    "f_new = f - learning_rate * grad_f\n",
    "\n",
    "# Display updated feature matrix\n",
    "f_df = pd.DataFrame(f_new, columns=[\"f1\", \"f2\", \"f3\"], index=[f\"x{i}\" for i in range(1, 6)])\n",
    "print(\"Updated Features (f):\")\n",
    "print(f_df)\n",
    "\n",
    "# Initial bias values\n",
    "b = np.array([0.15, -0.24, 0.31])\n",
    "\n",
    "# Gradient average for biases\n",
    "grad_b = np.array([0.181, -0.049, 0.160])\n",
    "\n",
    "# Update rule for biases\n",
    "b_new = b - learning_rate * grad_b\n",
    "\n",
    "# Display updated biases\n",
    "b_df = pd.DataFrame([b_new], columns=[\"b1\", \"b2\", \"b3\"], index=[\"Updated Bias\"])\n",
    "print(\"\\nUpdated Biases (b):\")\n",
    "print(b_df)\n"
   ],
   "id": "da8fbe6734f25f97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Features (f):\n",
      "       f1     f2     f3\n",
      "x1 -0.762 -0.614 -0.868\n",
      "x2  1.547 -0.640 -0.790\n",
      "x3 -0.762  1.805  1.492\n",
      "x4 -0.762  0.449 -0.790\n",
      "x5  1.530  0.020 -0.958\n",
      "\n",
      "Updated Biases (b):\n",
      "                 b1     b2    b3\n",
      "Updated Bias -0.031 -0.191  0.15\n"
     ]
    }
   ],
   "execution_count": 278
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:50:53.971475Z",
     "start_time": "2024-11-25T05:50:53.959482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Given data from the table\n",
    "import numpy as np\n",
    "\n",
    "# Derivatives of L_T\n",
    "delta_L_T = np.array([-0.125, 0.450, 0.231])\n",
    "\n",
    "# Derivatives of L_D\n",
    "delta_L_D = np.array([0.098, -0.343, -0.344])\n",
    "\n",
    "# Lambda value\n",
    "_lambda = -0.25\n",
    "\n",
    "# Formula: delta_L_T + lambda * delta_L_D\n",
    "final_values = delta_L_T + _lambda * delta_L_D\n",
    "\n",
    "# Compute the averages\n",
    "average = np.mean(final_values)\n",
    "\n",
    "final_values, average\n"
   ],
   "id": "92d244666bf3a018",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.1495 ,  0.53575,  0.317  ]), np.float64(0.23441666666666672))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:54:35.415714Z",
     "start_time": "2024-11-25T05:54:35.403940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initial weights from the table\n",
    "weights = np.array([\n",
    "    [0.127, -0.452, 0.889],\n",
    "    [-0.614, 0.294, -0.785],\n",
    "    [0.338, -0.726, 0.904]\n",
    "])\n",
    "\n",
    "# Gradients from the calculated row\n",
    "gradients = np.array([-0.1495, 0.53575, 0.317])\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 1\n",
    "\n",
    "# Update weights: w_new = w - alpha * gradient\n",
    "updated_weights = weights - learning_rate * gradients\n",
    "\n",
    "updated_weights\n"
   ],
   "id": "2fdd0684e87acc4b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2765 , -0.98775,  0.572  ],\n",
       "       [-0.4645 , -0.24175, -1.102  ],\n",
       "       [ 0.4875 , -1.26175,  0.587  ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T05:58:08.663005Z",
     "start_time": "2024-11-25T05:58:08.651853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Random gradients for δL_T/δb and δL_D/δb\n",
    "np.random.seed(42)  # For reproducibility\n",
    "delta_L_T_b = np.random.uniform(-1, 1, 3)  # Random values for δL_T/δb\n",
    "delta_L_D_b = np.random.uniform(-1, 1, 3)  # Random values for δL_D/δb\n",
    "\n",
    "# Lambda value\n",
    "_lambda = -0.25\n",
    "\n",
    "# Initial biases\n",
    "biases = np.array([0.15, -0.24, 0.31])\n",
    "\n",
    "# Compute weighted gradient: δL_T/δb + λ * δL_D/δb\n",
    "gradients_b = delta_L_T_b + _lambda * delta_L_D_b\n",
    "\n",
    "# Update biases using gradient descent\n",
    "learning_rate = 1\n",
    "updated_biases = biases - learning_rate * gradients_b\n",
    "\n",
    "# Calculate average (μ) of gradients\n",
    "average_gradient = np.mean(gradients_b)\n",
    "\n",
    "# Display results\n",
    "print(\"Initial Biases (b):\", biases)\n",
    "print(\"δL_T/δb:\", delta_L_T_b)\n",
    "print(\"δL_D/δb:\", delta_L_D_b)\n",
    "print(\"Weighted Gradients:\", gradients_b)\n",
    "print(\"Updated Biases (b):\", updated_biases)\n",
    "print(\"Average Gradient (μ):\", average_gradient)\n"
   ],
   "id": "bdff300035512d3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Biases (b): [ 0.15 -0.24  0.31]\n",
      "δL_T/δb: [-0.25091976  0.90142861  0.46398788]\n",
      "δL_D/δb: [ 0.19731697 -0.68796272 -0.68801096]\n",
      "Weighted Gradients: [-0.300249    1.07341929  0.63599062]\n",
      "Updated Biases (b): [ 0.450249   -1.31341929 -0.32599062]\n",
      "Average Gradient (μ): 0.46972030388317654\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T06:11:07.542625Z",
     "start_time": "2024-11-25T06:11:07.514424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assign the initial f values from the table\n",
    "f_values = np.array([\n",
    "    [-0.577, -0.864, -0.629],\n",
    "    [1.732, -0.890, -0.551],\n",
    "    [-0.577, 1.555, 1.731],\n",
    "    [-0.577, 0.199, -0.551],\n",
    "    [1.715, -0.230, -0.719]\n",
    "])\n",
    "\n",
    "# Random gradients for δL_T/δf and δL_D/δf\n",
    "np.random.seed(42)  # For reproducibility\n",
    "delta_L_T_f = np.random.uniform(-1, 1, (5, 3))  # Random values for δL_T/δf\n",
    "delta_L_D_f = np.random.uniform(-1, 1, (5, 3))  # Random values for δL_D/δf\n",
    "print(\"LT\")\n",
    "print(delta_L_T_f.mean(axis=0))\n",
    "print(\"LD\")\n",
    "print(delta_L_D_f.mean(axis=0))\n",
    "\n",
    "# Lambda value\n",
    "_lambda = -0.25\n",
    "\n",
    "# Compute weighted gradient: δL_T/δf + λ * δL_D/δf\n",
    "gradients_f = delta_L_T_f + _lambda * delta_L_D_f\n",
    "print(\"GradF\")\n",
    "print(gradients_f.mean(axis=0))\n",
    "\n",
    "# Update f values using gradient descent\n",
    "learning_rate = 1\n",
    "# Calculate averages (μ) of gradients for each column\n",
    "average_gradients = np.mean(gradients_f, axis=0)\n",
    "updated_f_values = f_values - learning_rate * average_gradients\n",
    "print(\"updatedF\")\n",
    "print(updated_f_values)\n",
    "\n",
    "\n",
    "# Display results\n",
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    \"Initial f\": f_values.flatten(),\n",
    "    \"δL_T/δf\": delta_L_T_f.flatten(),\n",
    "    \"δL_D/δf\": delta_L_D_f.flatten(),\n",
    "    \"Weighted Gradient\": gradients_f.flatten(),\n",
    "    \"Updated f\": updated_f_values.flatten()\n",
    "})\n",
    "\n",
    "# Display the calculated averages\n",
    "average_gradients, df_results\n"
   ],
   "id": "fc870f40bdeb5a07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LT\n",
      "[ 0.02871897 -0.11766692  0.05633532]\n",
      "LD\n",
      "[-0.30994088 -0.09391738 -0.30036185]\n",
      "GradF\n",
      "[ 0.10620419 -0.09418758  0.13142578]\n",
      "updatedF\n",
      "[[-0.68320419 -0.76981242 -0.76042578]\n",
      " [ 1.62579581 -0.79581242 -0.68242578]\n",
      " [-0.68320419  1.64918758  1.59957422]\n",
      " [-0.68320419  0.29318758 -0.68242578]\n",
      " [ 1.60879581 -0.13581242 -0.85042578]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.10620419, -0.09418758,  0.13142578]),\n",
       "     Initial f   δL_T/δf   δL_D/δf  Weighted Gradient  Updated f\n",
       " 0      -0.577 -0.250920 -0.633191          -0.092622  -0.683204\n",
       " 1      -0.864  0.901429 -0.391516           0.999307  -0.769812\n",
       " 2      -0.629  0.463988  0.049513           0.451610  -0.760426\n",
       " 3       1.732  0.197317 -0.136110           0.231344   1.625796\n",
       " 4      -0.890 -0.687963 -0.417542          -0.583577  -0.795812\n",
       " 5      -0.551 -0.688011  0.223706          -0.743937  -0.682426\n",
       " 6      -0.577 -0.883833 -0.721012          -0.703580  -0.683204\n",
       " 7       1.555  0.732352 -0.415711           0.836280   1.649188\n",
       " 8       1.731  0.202230 -0.267276           0.269049   1.599574\n",
       " 9      -0.577  0.416145 -0.087860           0.438110  -0.683204\n",
       " 10      0.199 -0.958831  0.570352          -1.101419   0.293188\n",
       " 11     -0.551  0.939820 -0.600652           1.089983  -0.682426\n",
       " 12      1.715  0.664885  0.028469           0.657768   1.608796\n",
       " 13     -0.230 -0.575322  0.184829          -0.621529  -0.135812\n",
       " 14     -0.719 -0.636350 -0.907099          -0.409575  -0.850426)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T06:52:37.134559Z",
     "start_time": "2024-11-27T06:52:36.908423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data from z and a tables\n",
    "z_values = np.array([\n",
    "    [0.384, 1.312, -0.718],\n",
    "    [-1.106, -0.201, 2.622],\n",
    "    [-0.820, -0.827, -0.780],\n",
    "    [1.776, 2.672, -0.975],\n",
    "    [0.632, 1.242, -0.394]\n",
    "])\n",
    "\n",
    "a_values = np.array([\n",
    "    [-0.327, 0.583, 0.814],\n",
    "    [0.494, 1.801, 0.285],\n",
    "    [-1.395, 0.692, 1.746],\n",
    "    [-0.410, 0.653, 0.526],\n",
    "    [0.434, 0.465, 0.696]\n",
    "])\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mu_z = np.mean(z_values, axis=1)\n",
    "sigma_z = np.std(z_values, axis=1, ddof=0)\n",
    "mu_a = np.mean(a_values, axis=1)\n",
    "sigma_a = np.std(a_values, axis=1, ddof=0)\n",
    "\n",
    "# Combine into a DataFrame\n",
    "results_fixed = pd.DataFrame({\n",
    "    \"μ_z\": mu_z,\n",
    "    \"σ_z\": sigma_z,\n",
    "    \"μ_a\": mu_a,\n",
    "    \"σ_a\": sigma_a\n",
    "}, index=[f\"x{i+1}\" for i in range(len(z_values))])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results_fixed)\n"
   ],
   "id": "c47ebccdf9b4ed33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         μ_z       σ_z       μ_a       σ_a\n",
      "x1  0.326000  0.829758  0.356667  0.492538\n",
      "x2  0.438333  1.587673  0.860000  0.670836\n",
      "x3 -0.809000  0.020704  0.347667  1.305219\n",
      "x4  1.157667  1.551753  0.256333  0.474013\n",
      "x5  0.493333  0.675053  0.531667  0.116888\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "def cover_zeros(matrix):\n",
    "    \"\"\"\n",
    "    Determines the minimum number of rows and columns to cover all zeros\n",
    "    in a square matrix using the Hungarian method.\n",
    "\n",
    "    Args:\n",
    "        matrix (numpy.ndarray): Input square matrix.\n",
    "\n",
    "    Returns:\n",
    "        tuple: List of selected rows, List of selected columns\n",
    "    \"\"\"\n",
    "    # Convert the matrix into a binary matrix (0 if zero, 1 otherwise)\n",
    "    binary_matrix = (matrix == 0).astype(int)\n",
    "\n",
    "    # Step 1: Initialize row and column covers\n",
    "    row_cover = np.zeros(matrix.shape[0], dtype=bool)\n",
    "    col_cover = np.zeros(matrix.shape[1], dtype=bool)\n",
    "\n",
    "    while True:\n",
    "        # Find rows with exactly one uncovered zero\n",
    "        for i, row in enumerate(binary_matrix):\n",
    "            if np.sum(row * (~col_cover)) == 1 and not row_cover[i]:\n",
    "                # Cover the row and uncover the column of the zero\n",
    "                row_cover[i] = True\n",
    "                zero_col = np.where((row * (~col_cover)) == 1)[0][0]\n",
    "                col_cover[zero_col] = False\n",
    "\n",
    "        # Find columns with exactly one uncovered zero\n",
    "        for j, col in enumerate(binary_matrix.T):\n",
    "            if np.sum(col * (~row_cover)) == 1 and not col_cover[j]:\n",
    "                # Cover the column and uncover the row of the zero\n",
    "                col_cover[j] = True\n",
    "                zero_row = np.where((col * (~row_cover)) == 1)[0][0]\n",
    "                row_cover[zero_row] = False\n",
    "\n",
    "        # Stop when no more rows/columns can be covered\n",
    "        if not (np.sum(~row_cover) + np.sum(~col_cover)):\n",
    "            break\n",
    "\n",
    "    # Return the selected rows and columns\n",
    "    selected_rows = np.where(row_cover)[0].tolist()\n",
    "    selected_columns = np.where(col_cover)[0].tolist()\n",
    "    return selected_rows, selected_columns\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "matrix = np.array([\n",
    "    [0, 1, 2],\n",
    "    [1, 0, 0],\n",
    "    [3, 2, 0]\n",
    "])\n",
    "\n",
    "selected_rows, selected_columns = cover_zeros(matrix)\n",
    "\n",
    "print(\"Selected Rows:\", selected_rows)\n",
    "print(\"Selected Columns:\", selected_columns)\n"
   ],
   "id": "2c8610e495901848"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T08:55:19.843890Z",
     "start_time": "2024-11-28T08:55:19.841431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "W = np.array([\n",
    "    0.12, -0.45, -.88,\n",
    "    -0.61, 0.29, -0.78,\n",
    "    0.33, -0.72, 0.90\n",
    "])\n",
    "\n"
   ],
   "id": "76000b001caaa84c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T08:59:02.292955Z",
     "start_time": "2024-11-28T08:59:02.284974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the ELU activation function and its derivative\n",
    "def elu(x, alpha=1):\n",
    "    return np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def elu_derivative(x, alpha=1):\n",
    "    return np.where(x >= 0, 1, alpha * np.exp(x))\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Define the cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / n  # Add epsilon for numerical stability\n",
    "    return loss\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(F, W, b, y_true):\n",
    "    # Linear transformation: Z = WF.T + b\n",
    "    Z = np.dot(W, F.T) + b.reshape(-1, 1)\n",
    "\n",
    "    # Apply ELU activation: A = ELU(Z)\n",
    "    A = elu(Z)\n",
    "\n",
    "    # Apply softmax: Y_hat = softmax(A.T)\n",
    "    Y_hat = softmax(A.T)\n",
    "\n",
    "    # Compute loss: L = cross-entropy(y_true, Y_hat)\n",
    "    loss = cross_entropy_loss(y_true, Y_hat)\n",
    "\n",
    "    return Z, A, Y_hat, loss\n",
    "\n",
    "# Backward pass\n",
    "def backward_pass(F, W, b, Z, A, Y_hat, y_true):\n",
    "    # Gradient w.r.t. softmax input (A)\n",
    "    dA = Y_hat - y_true  # Shape: (5, 3)\n",
    "\n",
    "    # Gradient w.r.t. ELU input (Z)\n",
    "    dZ = dA.T * elu_derivative(Z)  # Shape: (3, 5)\n",
    "\n",
    "    # Gradient w.r.t. weights (W)\n",
    "    dW = np.dot(dZ, F)  # Shape: (3, 3)\n",
    "\n",
    "    # Gradient w.r.t. biases (b)\n",
    "    db = np.sum(dZ, axis=1)  # Shape: (3,)\n",
    "\n",
    "    # Gradient w.r.t. inputs (F)\n",
    "    dF = np.dot(dZ.T, W)  # Shape: (5, 3)\n",
    "\n",
    "    return dW, db, dF\n",
    "\n",
    "# Initialize data\n",
    "F = np.array([\n",
    "    [-0.577, -0.864, -0.629],\n",
    "    [1.732, -0.890, -0.551],\n",
    "    [-0.577, 1.555, 1.731],\n",
    "    [-0.577, 0.199, -0.551],\n",
    "    [1.715, -0.230, -0.719]\n",
    "])  # Shape: (5, 3)\n",
    "\n",
    "W = np.array([\n",
    "    [0.12, -0.45, 0.88],\n",
    "    [-0.61, 0.29, -0.78],\n",
    "    [0.33, -0.72, 0.90]\n",
    "])  # Shape: (3, 3)\n",
    "\n",
    "b = np.array([0.15, -0.24, 0.31])  # Shape: (3,)\n",
    "\n",
    "y_true = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0]\n",
    "])  # Shape: (5, 3)\n",
    "\n",
    "# Perform forward pass\n",
    "Z, A, Y_hat, loss = forward_pass(F, W, b, y_true)\n",
    "\n",
    "# Perform backward pass\n",
    "dW, db, dF = backward_pass(F, W, b, Z, A, Y_hat, y_true)\n",
    "\n",
    "# Display results\n",
    "print(\"Forward Pass:\")\n",
    "print(f\"Z (Linear Output):\\n{Z}\")\n",
    "print(f\"A (ELU Output):\\n{A}\")\n",
    "print(f\"Y_hat (Softmax Output):\\n{Y_hat}\")\n",
    "print(f\"Loss: {loss}\\n\")\n",
    "\n",
    "print(\"Backward Pass:\")\n",
    "print(f\"dW (Gradient w.r.t. Weights):\\n{dW}\")\n",
    "print(f\"db (Gradient w.r.t. Biases):\\n{db}\")\n",
    "print(f\"dF (Gradient w.r.t. Inputs):\\n{dF}\")\n"
   ],
   "id": "74fd0dc27ea6cc79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass:\n",
      "Z (Linear Output):\n",
      "[[-0.08396  0.27346  0.90429 -0.49367 -0.17342]\n",
      " [ 0.35203 -1.12484 -0.78726  0.59946 -0.79203]\n",
      " [ 0.17557  1.02646  0.55789 -0.51959  0.39445]]\n",
      "A (ELU Output):\n",
      "[[-0.08053197  0.27346     0.90429    -0.38961782 -0.15921559]\n",
      " [ 0.35203    -0.67529558 -0.54490996  0.59946    -0.54707558]\n",
      " [ 0.17557     1.02646     0.55789    -0.40523565  0.39445   ]]\n",
      "Y_hat (Softmax Output):\n",
      "[[0.26088644 0.40207843 0.33703513]\n",
      " [0.28485296 0.1103015  0.60484554]\n",
      " [0.51493636 0.12088544 0.3641782 ]\n",
      " [0.21398351 0.57534898 0.21066751]\n",
      " [0.29255802 0.19850276 0.50893922]]\n",
      "Loss: 1.543425674830464\n",
      "\n",
      "Backward Pass:\n",
      "dW (Gradient w.r.t. Weights):\n",
      "[[ 1.28705143  0.98232446  1.24935895]\n",
      " [-1.71864967  0.19325038 -0.05450824]\n",
      " [ 2.02052647 -1.91033559 -2.08083876]]\n",
      "db (Gradient w.r.t. Biases):\n",
      "[-0.11359421  0.38053446  0.94029562]\n",
      "dF (Gradient w.r.t. Inputs):\n",
      "[[-0.21559721  0.17975353 -0.6083299 ]\n",
      " [ 0.41000369 -0.64745044  1.02036503]\n",
      " [-0.18158722  0.24202433 -0.16200636]\n",
      " [-0.36718715  0.29253369 -0.75820243]\n",
      " [ 0.41890811 -0.58240156  0.95765992]]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:41:37.715154Z",
     "start_time": "2024-11-28T09:41:37.710275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "delta_Z = np.array([\n",
    "    [ 0.325, -0.507,  0.157],\n",
    "    [-0.218,  0.335,  0.327],\n",
    "    [ 0.144,  0.144, -0.227],\n",
    "    [ 0.334,  0.327, -0.219],\n",
    "    [ 0.322, -0.546,  0.764]\n",
    "])\n",
    "F = np.array([\n",
    "    [-0.577, -0.864, -0.629],\n",
    "    [ 1.732, -0.890, -0.551],\n",
    "    [-0.577,  1.555,  1.731],\n",
    "    [-0.577,  0.199, -0.551],\n",
    "    [ 1.715, -0.230, -0.719]\n",
    "])\n",
    "\n",
    "W = np.array([\n",
    "    [ 0.12, -0.45,  0.88],\n",
    "    [-0.61,  0.29, -0.78],\n",
    "    [ 0.33, -0.72,  0.90]\n",
    "])\n",
    "\n",
    "print(W.T)\n",
    "\n",
    "b = np.array([0.15, -0.24, 0.31])\n",
    "\n",
    "# Gradients\n",
    "# 1. Gradient w.r.t. Bias (b)\n",
    "db = np.sum(delta_Z, axis=0)\n",
    "\n",
    "# 2. Gradient w.r.t. Weights (W)\n",
    "dW = delta_Z.T @ F\n",
    "\n",
    "# 3. Gradient w.r.t. Inputs (F)\n",
    "dF = delta_Z @ W\n",
    "\n",
    "# Results\n",
    "# print(\"Gradient w.r.t. Bias (db):\\n\", db)\n",
    "# print(\"\\nGradient w.r.t. Weights (dW):\\n\", dW)\n",
    "print(\"\\nGradient w.r.t. Inputs (dF):\\n\", dF)\n",
    "print(dF.shape)\n",
    "print(dF.mean(axis=0))\n"
   ],
   "id": "65305f3d4791e0c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12 -0.61  0.33]\n",
      " [-0.45  0.29 -0.72]\n",
      " [ 0.88 -0.78  0.9 ]]\n",
      "\n",
      "Gradient w.r.t. Inputs (dF):\n",
      " [[ 0.40008 -0.40632  0.82276]\n",
      " [-0.1226  -0.04019 -0.15884]\n",
      " [-0.14547  0.1404  -0.1899 ]\n",
      " [-0.23166  0.10221 -0.15824]\n",
      " [ 0.62382 -0.85332  1.39684]]\n",
      "(5, 3)\n",
      "[ 0.104834 -0.211444  0.342524]\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
